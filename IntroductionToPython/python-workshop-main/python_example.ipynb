{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Project: Comparing Trump's and Biden's Inaugural Speeches\n",
    "\n",
    "We will use a mini-project as an extended practical example to demonstrate the concepts we are learning in the workshop. The project aims to analyze and compare the inaugural speeches of the current and last US presidents.\n",
    "\n",
    "The speech transcripts were obtained from https://millercenter.org/the-presidency/presidential-speeches and copied in the text files `biden_inauguration_millercenter.txt` and `trump_inauguration_millercenter.txt` in the `data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Straight-line programming\n",
    "\n",
    "Even just with basic understanding of data types, operations, and methods, we can already extract useful information from data. Below, we will:\n",
    "1. Open the text file with one of the speeches\n",
    "2. Clean up the text and extract a list of all the words used in the speech\n",
    "3. Estimate the length of the speach and number of unique words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans, and people of the world: thank you.\\n\\nWe, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all of our people.\\n\\nTogether, we will determine the course of America and the world for years to come.\\n\\nWe will face challenges. We will confront hardships. But we will get the job done.\\n\\nEvery four years, we gather on these st'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file and get the text into a string variable called txt\n",
    "with open('data/trump_inauguration_millercenter.txt') as f:\n",
    "    txt = f.read()\n",
    "txt[:500] # Show the first 500 characters of the txt variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017', '20th', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'about', 'about', 'accept', 'across', 'across', 'across', 'across', 'across', 'action', 'action', 'administration', 'affairs', 'again', 'again', 'again', 'again', 'again', 'again', 'again', 'again', 'again', 'against', 'aid', 'airports', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'allegiance', 'allegiance', 'alliances', 'allowing', 'almighty', 'along', 'always', 'always', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'america', 'american', 'american', 'american', 'american', 'american', 'american', 'american', 'american', 'american', 'american', 'american', 'american', 'americans', 'americans', 'americans', 'americans', 'an', 'an', 'an', 'and', 'and']\n",
      "1436\n",
      "536\n"
     ]
    }
   ],
   "source": [
    "# Remove paragraphs and format consistently\n",
    "txt = txt.strip().replace('\\n', ' ').replace(\"’\", \"'\")\n",
    "\n",
    "# Get rid of possessives and expand contractions\n",
    "txt = txt.replace(\"'s\", '').replace(\"'ve\", ' have').replace(\"'re\", ' are')\n",
    "txt = txt.replace(\"can't\", 'can not').replace(\"n't\", ' not')\n",
    "\n",
    "# Remove punctuation\n",
    "txt = txt.replace('—', '').replace('–', '')\n",
    "txt = txt.replace('.', '').replace(',', '').replace(':', '').replace(';', '').replace('…', '')\n",
    "txt = txt.replace(\"”\", '').replace(\"“\", '')\n",
    "\n",
    "# Convert to lower-case\n",
    "txt = txt.lower()\n",
    "\n",
    "# Break into words\n",
    "wrds = txt.split()\n",
    "print(sorted(wrds)[:100])\n",
    "\n",
    "# Count the number of words in the speech\n",
    "print(len(wrds))\n",
    "\n",
    "# Count the number of unique words\n",
    "print(len(set(wrds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Control flow with conditionals and loops\n",
    "\n",
    "Branching and iteration allow us to employ more complex logic in our data processing and analysis: e.g., repeat operations or set conditions to select data. Below, we will:\n",
    "1. Count the number of times each unique word is mentioned in the speech\n",
    "2. Exclude non-meaningful words such as articles and prepositions\n",
    "3. Identify the most commonly used meaningful words to reveal the theme and tone of the speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 74),\n",
       " ('the', 70),\n",
       " ('we', 49),\n",
       " ('of', 48),\n",
       " ('our', 48),\n",
       " ('will', 40),\n",
       " ('to', 37),\n",
       " ('is', 21),\n",
       " ('america', 18),\n",
       " ('a', 15)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary with word:count\n",
    "word_counts = {}\n",
    "\n",
    "for i in wrds:\n",
    "    if i not in word_counts:\n",
    "        word_counts[i] = 1\n",
    "    else:\n",
    "        word_counts[i] += 1\n",
    "\n",
    "# Print the words with counts in decreasing order of popularity\n",
    "# Note this produces a list of tuples\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda i: i[1], reverse=True)\n",
    "\n",
    "sorted_word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 49),\n",
       " ('our', 48),\n",
       " ('will', 40),\n",
       " ('america', 18),\n",
       " ('you', 12),\n",
       " ('all', 12),\n",
       " ('american', 12),\n",
       " ('their', 11),\n",
       " ('your', 11),\n",
       " ('people', 9)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will create a dictionary of all words mentioned more than once without stop words\n",
    "# Stop words are common words that are not meaningful in this context\n",
    "stop_words = ['a', 'about', 'across', 'after', 'an', 'and', 'any', 'are', 'as', 'at', \n",
    "              'be', 'because', 'but', 'by', 'did', 'do', 'does', 'for', 'from',\n",
    "              'get', 'has', 'have', 'if', 'in', 'is', 'it', 'its',\n",
    "              'many', 'more', 'much', 'no', 'not', 'of', 'on', 'or', 'out',\n",
    "              'so', 'some', 'than', 'the', 'this', 'that', 'those', 'through', 'to',\n",
    "              'very', 'what', 'where', 'whether', 'which', 'while', 'who', 'with']\n",
    "\n",
    "common_words = []\n",
    "for i in sorted_word_counts:\n",
    "    if i[0] not in stop_words:\n",
    "        if i[1] > 1:\n",
    "            common_words.append(i)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "# Alternatively, we can use a list comprehension for the code block above\n",
    "# common_words = [i for i in sorted_word_counts if i[0] not in stop_words and i[1] > 1]     \n",
    "        \n",
    "common_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functions\n",
    "\n",
    "Once we understand conditionals, loops, and functions, we can improve the code above and make it more efficient and modular. This will allow us to apply it to multiple data files, without the need to duplicate large chunks of code. Below, we will:\n",
    "1. Create a function to extract words from text and another function to count words in a text\n",
    "2. Apply the functions to each president's speech\n",
    "3. Compare the length and repetitiveness of the speeches, the most common words and the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chief', 'justice', 'roberts', 'president', 'carter', 'president', 'clinton', 'president', 'bush', 'president', 'obama', 'fellow', 'americans', 'and', 'people', 'of', 'the', 'world', 'thank', 'you', 'we', 'the', 'citizens', 'of', 'america', 'are', 'now', 'joined', 'in', 'a', 'great', 'national', 'effort', 'to', 'rebuild', 'our', 'country', 'and', 'to', 'restore', 'its', 'promise', 'for', 'all', 'of', 'our', 'people', 'together', 'we', 'will', 'determine', 'the', 'course', 'of', 'america', 'and', 'the', 'world', 'for', 'years', 'to', 'come', 'we', 'will', 'face', 'challenges', 'we', 'will', 'confront', 'hardships', 'but', 'we', 'will', 'get', 'the', 'job', 'done', 'every', 'four', 'years', 'we', 'gather', 'on', 'these', 'steps', 'to', 'carry', 'out', 'the', 'orderly', 'and', 'peaceful', 'transfer', 'of', 'power', 'and', 'we', 'are', 'grateful', 'to']\n"
     ]
    }
   ],
   "source": [
    "import string  # See https://docs.python.org/3/library/string.html\n",
    "\n",
    "# This will now be a global variable so we will follow the convention and \n",
    "# name it in all caps\n",
    "STOP_WORDS = ['a', 'about', 'across', 'after', 'an', 'and', 'any', 'are', 'as', 'at', \n",
    "              'be', 'because', 'but', 'by', 'did', 'do', 'does', 'for', 'from',\n",
    "              'get', 'has', 'have', 'if', 'in', 'is', 'it', 'its',\n",
    "              'many', 'more', 'much', 'no', 'not', 'of', 'on', 'or', 'out',\n",
    "              'so', 'some', 'than', 'the', 'this', 'that', 'those', 'through', 'to',\n",
    "              'very', 'what', 'where', 'whether', 'which', 'while', 'who', 'with']\n",
    "\n",
    "def get_tokens(fname):\n",
    "    \"\"\"Read given text file and return a list with all words in lowercase\n",
    "    in the order they appear in the text. Common contractions are expanded\n",
    "    and hyphenated words are combined in one word.\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        txt = f.read()\n",
    "        \n",
    "    # Remove paragraphs and format consistently\n",
    "    txt = txt.strip().replace('\\n', ' ').replace(\"’\", \"'\")\n",
    "    \n",
    "    # Get rid of possessives and expand contractions\n",
    "    txt = txt.replace(\"'s\", '').replace(\"'ve\", ' have').replace(\"'re\", ' are')\n",
    "    txt = txt.replace(\"can't\", 'can not').replace(\"n't\", ' not')\n",
    "\n",
    "    # Remove punctuation and convert to lower-case\n",
    "    exclude = set(string.punctuation) | {\"”\", \"“\", \"…\", '–'}\n",
    "    txt = ''.join(ch.lower() for ch in txt if ch not in exclude)\n",
    "\n",
    "    # Break into words\n",
    "    wrds = txt.split()\n",
    "    \n",
    "    return wrds\n",
    "\n",
    "\n",
    "def get_word_counts(tokens):\n",
    "    \"\"\"Take tokens and return a dictionary where keys are words\n",
    "    and values are counts of the number of time the word is repeated.\n",
    "    \"\"\"\n",
    "    # Create dictionary with word:count\n",
    "    word_counts = {}\n",
    "\n",
    "    for i in tokens:\n",
    "        if i not in STOP_WORDS:\n",
    "            if i not in word_counts:\n",
    "                word_counts[i] = 1\n",
    "            else:\n",
    "                word_counts[i] += 1\n",
    "\n",
    "    # Get the words with counts in decreasing order of popularity\n",
    "    # Note this produces a list of tuples\n",
    "    sorted_word_counts = sorted(word_counts.items(), key=lambda i: i[1], reverse=True)\n",
    "    \n",
    "    return sorted_word_counts\n",
    "\n",
    "\n",
    "trump_tokens = get_tokens('data/trump_inauguration_millercenter.txt')\n",
    "biden_tokens = get_tokens('data/biden_inauguration_millercenter.txt')\n",
    "\n",
    "print(trump_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1436 2382\n",
      "536 721\n",
      "2.6791044776119404 3.30374479889043\n",
      "\n",
      "[('we', 49), ('our', 48), ('will', 40), ('america', 18), ('you', 12), ('all', 12), ('american', 12), ('their', 11), ('your', 11), ('people', 9), ('country', 9), ('nation', 9), ('again', 9), ('one', 8), ('every', 7), ('world', 6), ('now', 6), ('great', 6), ('back', 6), ('never', 6)]\n",
      "\n",
      "[('we', 91), ('our', 43), ('will', 33), ('i', 33), ('us', 27), ('my', 20), ('america', 20), ('can', 18), ('you', 17), ('all', 17), ('one', 15), ('nation', 14), ('democracy', 11), ('me', 11), ('must', 10), ('americans', 9), ('today', 9), ('people', 9), ('american', 9), ('story', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Biden's speech is longer\n",
    "print(len(trump_tokens), len(biden_tokens))\n",
    "print(len(set(trump_tokens)), len(set(biden_tokens)))\n",
    "\n",
    "# Biden's speech is also more repetitive\n",
    "print(len(trump_tokens)/len(set(trump_tokens)), len(biden_tokens)/len(set(biden_tokens)))\n",
    "\n",
    "print() # Add an empty line to separate results\n",
    "\n",
    "# The ten most common words for Trump and Biden\n",
    "trump_wcounts = get_word_counts(trump_tokens)\n",
    "biden_wcounts = get_word_counts(biden_tokens)\n",
    "\n",
    "# Biden's speech is more self-centered\n",
    "print(trump_wcounts[:20])\n",
    "\n",
    "print() # Add an empty line to separate results\n",
    "\n",
    "print(biden_wcounts[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('back', 6), ('protected', 5), ('dreams', 5), ('wealth', 4), ('everyone', 4), ('bring', 4), ('obama', 3), ('too', 3), ('capital', 3), ('government', 3), ('factories', 3), ('foreign', 3), ('countries', 3)]\n",
      "\n",
      "[('democracy', 11), ('me', 11), ('story', 9), ('know', 8), ('history', 7), ('war', 7), ('days', 6), ('truth', 5), ('may', 5), ('cause', 4), ('centuries', 4), ('peace', 4), ('virus', 4), ('lost', 4), ('soul', 4), ('things', 4), ('once', 4), ('better', 4), ('need', 4), ('say', 4), ('vice', 3), ('hope', 3), ('resolve', 3), ('prevailed', 3), ('ago', 3), ('violence', 3), ('them', 3), ('constitution', 3), ('sacred', 3), ('year', 3), ('cry', 3), ('whole', 3), ('uniting', 3), ('join', 3), ('common', 3), ('faith', 3), ('show', 3), ('dignity', 3), ('respect', 3), ('meet', 3), ('believe', 3), ('yet', 3), ('gave', 3), ('honor', 3), ('lies', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Get repeated words and check the difference\n",
    "trump_100 = set([i[0] for i in trump_wcounts])\n",
    "biden_100 = set([i[0] for i in biden_wcounts])\n",
    "\n",
    "# Unique words only for Trump\n",
    "print([i for i in trump_wcounts if i[0] in (trump_100-biden_100) and i[1] > 2])\n",
    "\n",
    "print()\n",
    "\n",
    "# Unique words only for Biden\n",
    "print([i for i in biden_wcounts if i[0] in (biden_100- trump_100) and i[1] > 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classes\n",
    "\n",
    "What we did above is known as procedural programming – we keep functions and data separate and pass the data to the functions. Alternatively, we can employ the approach of object-oriented programming – we can bundle up the data and functions into classes. In this case, the functions become methods and they belong only to this particular data type. We cannot call them independently, on other types of data, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans, and people of the world: thank you.\n",
      "\n",
      "We, the citizens of America, are now joined in a gre...\n",
      "1436\n",
      "\n",
      "Chief Justice Roberts, Vice President Harris, Speaker Pelosi, Leader Schumer, Leader McConnell, Vice President Pence, distinguished guests, and my fellow Americans.\n",
      "\n",
      "This is America’s day.\n",
      "\n",
      "This is de...\n",
      "2382\n"
     ]
    }
   ],
   "source": [
    "class Speech(object):\n",
    "        \n",
    "    def __init__(self, fname):\n",
    "        \"\"\"Creates a speech using the text in file fname.\"\"\"\n",
    "        \n",
    "        with open(fname) as f:\n",
    "            self.txt = f.read()\n",
    "        self.tokens = None\n",
    "        self.word_counts = None\n",
    "        \n",
    "        # Populate the empty attributes above by processing the text\n",
    "        self.process_tokens()        \n",
    "        self.process_word_counts()\n",
    "    \n",
    "    \n",
    "    # The following two methods are called when you initialize a new object\n",
    "        \n",
    "    def process_tokens(self):\n",
    "        \"\"\"Extracts the tokens in the text and assigns them to \n",
    "        the attribute 'tokens'. 'tokens' is a list of strings.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Remove paragraphs and format consistently\n",
    "        txt = self.txt.strip().replace('\\n', ' ').replace(\"’\", \"'\")\n",
    "        \n",
    "        # Get rid of possessives and expand contractions\n",
    "        txt = txt.replace(\"'s\", '').replace(\"'ve\", ' have').replace(\"'re\", ' are')\n",
    "        txt = txt.replace(\"can't\", 'can not').replace(\"n't\", ' not')\n",
    "    \n",
    "        # Remove punctuation and convert to lower-case\n",
    "        exclude = set(string.punctuation) | {\"”\", \"“\", \"…\", '–'}\n",
    "        txt = ''.join(ch.lower() for ch in txt if ch not in exclude)\n",
    "\n",
    "        # Break into words\n",
    "        wrds = txt.split()\n",
    "\n",
    "        self.tokens = wrds\n",
    "        \n",
    "        \n",
    "    def process_word_counts(self):\n",
    "        \"\"\"Counts the number of times each word, excluding stop words,\n",
    "        appears in the speech and assigns the counts to the attribute 'word_counts'.\n",
    "        'word_counts' is a list of tuples in the form (token, count).\n",
    "        \"\"\"\n",
    "        # Create dictionary with word:count\n",
    "        word_counts = {}\n",
    "\n",
    "        for i in self.tokens:\n",
    "            if i not in STOP_WORDS:\n",
    "                if i not in word_counts:\n",
    "                    word_counts[i] = 1\n",
    "                else:\n",
    "                    word_counts[i] += 1\n",
    "\n",
    "        # Get the words with counts in decreasing order of popularity\n",
    "        # Note this produces a list of tuples\n",
    "        sorted_word_counts = sorted(word_counts.items(), key=lambda i: i[1], reverse=True)\n",
    "        self.word_counts = sorted_word_counts\n",
    "    \n",
    "    \n",
    "    # Use get and set methods to provide interface for interacting with the objects\n",
    "        \n",
    "    def get_text():\n",
    "        return self.text\n",
    "        \n",
    "    def get_tokens(self):\n",
    "        \"\"\"Get the tokens in the speech as a list of strings.\"\"\"\n",
    "        # Avoid returning mutable objects as they could be modified in undesirable ways\n",
    "        return self.tokens[:]\n",
    "    \n",
    "    def get_word_counts(self):\n",
    "        \"\"\"Get each unique word in the speech and the number of times it appears in the speech.\n",
    "        Return a list of tuples in the form (token, count).\n",
    "        \"\"\"\n",
    "        # Avoid returning mutable objects as they could be modified in undesirable ways\n",
    "        return self.word_counts[:]\n",
    "    \n",
    "    # You can make your code even more interactive by providing extra methods for\n",
    "    # common and useful operations\n",
    "    \n",
    "    def get_speech_length(self):\n",
    "        \"\"\"Get the number of tokens in the speech.\"\"\"\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def get_number_unique_tokens(self):\n",
    "        \"\"\"Gets the number of unique words used in the speech,\n",
    "        including stop words.\n",
    "        \"\"\"\n",
    "        return len(set(self.tokens))\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Returns the first 200 characters of the speech.\"\"\"\n",
    "        return self.txt[:200] + '...'\n",
    "\n",
    "    \n",
    "# Create an object of class Speech for Trump's inaugural speech\n",
    "trump = Speech('data/trump_inauguration_millercenter.txt')\n",
    "print(trump)\n",
    "# Process the speech text and get the length of the speech\n",
    "print(trump.get_speech_length())\n",
    "\n",
    "print()\n",
    "\n",
    "# Create anothe Speech object for Biden's inaugural speech\n",
    "biden = Speech('data/biden_inauguration_millercenter.txt')\n",
    "print(biden)\n",
    "print(biden.get_speech_length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
